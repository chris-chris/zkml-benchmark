{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP Benchmark with ezkl Integration\n",
        "\n",
        "This notebook benchmarks an MLP model implemented in PyTorch, which mirrors the structure of an MLP model defined using o1js in TypeScript. The model is exported to ONNX format and integrated with ezkl for generating and verifying Zero-Knowledge Proofs (ZKP). The notebook is structured to automate the steps for proving using `ezkl prove`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install torch numpy onnx ezkl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import onnx\n",
        "import os\n",
        "import json\n",
        "import ezkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Experiment Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "exp_num = 5  # Adjust depth complexity here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "! mkdir -p mlp{exp_num}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = os.path.join(f'mlp{exp_num}/mlp.onnx')\n",
        "compiled_model_path = os.path.join(f'mlp{exp_num}/model.compiled')\n",
        "pk_path = os.path.join(f'mlp{exp_num}/pk.key')\n",
        "vk_path = os.path.join(f'mlp{exp_num}/test.vk')\n",
        "settings_path = os.path.join(f'mlp{exp_num}/settings.json')\n",
        "\n",
        "witness_path = os.path.join(f'mlp{exp_num}/witness.json')\n",
        "data_path = os.path.join('input.json')\n",
        "onnx_path =  os.path.join(f\"mlp{exp_num}/mlp.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MLP class with perceptron layers\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, depth):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.ModuleList([nn.Linear(5, 5) for _ in range(depth)])\n",
        "        self.output = nn.Linear(5, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.zeros_(m.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = self.relu(layer(x))\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model with adjustable depth\n",
        "depth = 2 ** exp_num\n",
        "model = MLP(depth=depth)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Input Data from input.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load input data from input.json\n",
        "def load_input_data(json_file):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return torch.tensor(data['input_data'], requires_grad=True)\n",
        "\n",
        "input_data = load_input_data('input.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Output: tensor([[0.0209],\n",
            "        [0.0209]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Perform forward pass through the network\n",
        "output = model(input_data)\n",
        "print(\"Model Output:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export the Model to ONNX Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model exported to mlp5/mlp.onnx\n"
          ]
        }
      ],
      "source": [
        "# Export the model to ONNX format\n",
        "torch.onnx.export(\n",
        "    model,                          # Model being run\n",
        "    input_data,                     # Model input (or a tuple for multiple inputs)\n",
        "    onnx_path,                      # Where to save the model (can be a file or file-like object)\n",
        "    export_params=True,             # Store the trained parameter weights inside the model file\n",
        "    opset_version=10,               # The ONNX version to export the model to\n",
        "    do_constant_folding=True,       # Whether to execute constant folding for optimization\n",
        "    input_names=['input'],          # The model's input names\n",
        "    output_names=['output'],        # The model's output names\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # Variable length axes\n",
        ")\n",
        "\n",
        "print(f\"Model exported to {onnx_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate the ONNX Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX model is valid\n"
          ]
        }
      ],
      "source": [
        "# Load and check the ONNX model\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print('ONNX model is valid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Settings for ezkl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "res = ezkl.gen_settings(model_path, settings_path)\n",
        "assert res == True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile the Circuit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
        "assert res == True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Proofing and Verification Keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the SRS (Structured Reference String) needed for zk-SNARKs\n",
        "res = ezkl.get_srs(settings_path)\n",
        "assert res == True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the proofing and verification keys\n",
        "res = ezkl.setup(compiled_model_path, vk_path, pk_path)\n",
        "assert res == True\n",
        "assert os.path.isfile(vk_path)\n",
        "assert os.path.isfile(pk_path)\n",
        "assert os.path.isfile(settings_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the Witness File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the witness file for proving\n",
        "res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
        "assert os.path.isfile(witness_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Proving Command\n",
        "\n",
        "Finally, the proof can be generated and verified using the following command:\n",
        "```bash\n",
        "ezkl prove --witness models/mlp/mlp$i/witness.json --pk-path models/mlp/mlp$i/pk.key --compiled-circuit models/mlp/mlp$i/model.compiled --proof-path models/mlp/mlp$i/proof.json\n",
        "```\n",
        "This command runs the proof generation using the witness file, proofing key, and compiled circuit."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

